{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# 1 - Adjust base data\n",
    "\n",
    "NetCDF data has already been combined and mapped to consistent depths. This Notebook will import data, truncate to desired depth range, check for data gaps and segment the dataset accordingly. Finally, data will be rotated according to site, small gaps interpolated, and filtered to acquire mean currents and residual flows. Output in NetCDF format for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Import modules and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as pldates\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import pandas as pd\n",
    "for i in range(2):\n",
    "    %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     5,
     8,
     11
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import 'raw' data\n",
    "\n",
    "adcp = 'Shoulder'     # Slope, Axis75, Axis55, Shoulder\n",
    "season = 'Annual'  # Annual, Winter, Spring, Summer, Fall\n",
    "\n",
    "if adcp == 'Slope':\n",
    "    ds = xr.open_dataset(f'../Data/{adcp}All/{adcp}_all.nc')\n",
    "    name = 'Slope'\n",
    "elif adcp == 'Axis75':\n",
    "    ds = xr.open_dataset(f'../Data/AxisAll/{adcp}/{adcp}_all.nc')\n",
    "    name = 'Axis'   # for consistent naming between Axis 75 and 55 kHz\n",
    "elif adcp == 'Axis55':\n",
    "    ds = xr.open_dataset(f'../Data/AxisAll/{adcp}/{adcp}_all.nc')\n",
    "    name = 'Axis'   # for consistent naming between Axis 75 and 55 kHz      \n",
    "\n",
    "elif adcp == 'Shoulder':\n",
    "    year = 2018\n",
    "    adcp2 = 'Axis55'         # Slope, Axis75, Axis55 \n",
    "    if adcp2 == 'Slope':\n",
    "        name = 'Slope (1-min)'   # for consistent naming\n",
    "    elif adcp2 == 'Axis75' or adcp2 == 'Axis55':\n",
    "        name = 'Axis (1-min)'\n",
    "    ds = xr.open_mfdataset(f'../Data/QA/1min/{adcp2}_{year}/*.nc',join='override')   \n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot dataset to check data quality\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(13,5))\n",
    "\n",
    "# if adcp2 == 'Axis75':\n",
    "#     im = ax.pcolormesh(ds.time, -ds.depth, ds.u[0,0,:,:].T, rasterized=True, cmap='RdBu_r', vmin=-0.15, vmax=0.15, shading='auto')\n",
    "# else:\n",
    "#     im = ax.pcolormesh(ds.time, -ds.depth, ds.u.T, rasterized=True, cmap='RdBu_r', vmin=-0.15, vmax=0.15, shading='auto')\n",
    "# cbar = fig.colorbar(im, ax=ax, fraction=0.05, pad=0.01, aspect=40, extend='both')\n",
    "# cbar.set_label('Velocity [m/s]')\n",
    "# ax.set_xlabel('Time')\n",
    "# ax.set_ylabel('Depth [m]')\n",
    "# ax.set_title('Velocity data')\n",
    "# date_form = pldates.DateFormatter(\"%m\")\n",
    "# ax.xaxis.set_major_formatter(date_form)\n",
    "# #ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Depth\n",
    "\n",
    "Truncate data to a specific depth interval to eliminate unreliable data, based on data quality checks regarding correlation and backscatter intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0,
     2,
     8,
     11,
     14
    ]
   },
   "outputs": [],
   "source": [
    "# process to find nearby indices for desired depth values\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx      # returns index of nearest depth value\n",
    "\n",
    "array = ds.depth         # input array to process \n",
    "if adcp=='Slope':\n",
    "    upval = 0         # upper depth for data, metres (upper slope)\n",
    "    lowval = 400        # lower depth for data (upper slope)\n",
    "elif adcp=='Axis75':\n",
    "    upval = 0         # upper depth for data, metres (axis)\n",
    "    lowval = 1000        # lower depth for data (axis)\n",
    "elif adcp=='Axis55':\n",
    "    upval = 0            # upper depth for data, metres (axis)\n",
    "    lowval = 1000        # lower depth for data (axis)\n",
    "\n",
    "if adcp2=='Slope':\n",
    "    upval = 0            # upper depth for data, metres (axis)\n",
    "    lowval = 400        # lower depth for data (axis)\n",
    "elif adcp2=='Axis75':\n",
    "    upval = 0\n",
    "    lowval = 1000\n",
    "elif adcp2=='Axis55':\n",
    "    upval = 10\n",
    "    lowval = 1000\n",
    "    \n",
    "upidx = find_nearest(array, upval)                 # index of upper depth cutoff\n",
    "lowidx = find_nearest(array,lowval)                # index of lower depth cutoff \n",
    "\n",
    "print(\"Index at upper depth cutoff:\",upidx,\"/ Value at upper depth cutoff:\",-ds.depth.values[upidx],\"metres\")\n",
    "print(\"Index at lower depth cutoff:\",lowidx,\"/ Value at lower depth cutoff:\",-ds.depth.values[lowidx],\"metres\")\n",
    "\n",
    "if adcp2=='Slope' or adcp2=='Axis75' or adcp2=='Axis55':\n",
    "    depth = np.array(ds.depth[lowidx:upidx+1])         # remove unwanted depths\n",
    "    dup_stamp = int(-depth[-1])                         # depth stamps for use in output filenames\n",
    "    dlow_stamp = int(-depth[0])\n",
    "    print(\"Length of new depth array: \",len(depth),'/ Upper limit:',dup_stamp,'metres','/ Lower limit:',dlow_stamp,'metres')   # new depth interval\n",
    "else:\n",
    "    depth = np.array(ds.depth[upidx:lowidx+1])         # remove unwanted depths\n",
    "    dup_stamp = int(-depth[0])                         # depth stamps for use in output filenames\n",
    "    dlow_stamp = int(-depth[-1])\n",
    "    print(\"Length of new depth array: \",len(depth),'/ Upper limit:',dup_stamp,'metres','/ Lower limit:',dlow_stamp,'metres')   # new depth interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Time\n",
    "\n",
    "Find specific time range and format dates. If significant *consecutive* NaN values are present, then split time series into segments for analysis processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set date range\n",
    "\n",
    "year = 2018\n",
    "\n",
    "datestimes = pd.to_datetime(ds.time.values)                          # convert to datetime from datetime64\n",
    "datestimes = pd.Series(datestimes)                                   # convert to pandas dataframe\n",
    "\n",
    "start_date = dt.datetime(year,1,1)                                   # input start date in YYYY,MM,DD\n",
    "end_date = dt.datetime(year+1,1,1)                                   # input end date in YYYY,MM,DD\n",
    "start = datestimes[datestimes >= start_date].index[0]                # desired start date\n",
    "end = datestimes[datestimes < end_date].index[-1]                    # desired end date\n",
    "time_test = ds.time.values[start:end]                                # test desired interval\n",
    "t_stamp = f'{datestimes.dt.year[start]}'                             # set year time stamp for output filenames\n",
    "print(\"Desired time range:\",np.min(time_test),np.max(time_test))     # print to check desired interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# check time series for significant data gaps\n",
    "\n",
    "if adcp == 'Slope' or adcp2 == 'Slope':\n",
    "    depth_test = ds.depth[lowidx-15]        # mid depth for NaN testing\n",
    "elif adcp2 == 'Axis55':\n",
    "    depth_test = ds.depth[10]\n",
    "else:\n",
    "    depth_test = ds.depth[15]\n",
    "print('Checked at depth',-depth_test.values,'m')\n",
    "\n",
    "if adcp2 == 'Axis75':\n",
    "    u_test = np.array(ds.u[0,0,start:end,15])            # u data at this depth\n",
    "elif adcp2 == 'Axis55' and year == 2017:\n",
    "    u_test = np.array(ds.u[start:end,10])\n",
    "elif adcp2 == 'Axis55' and year == 2018:\n",
    "    u_test = np.array(ds.u[0,0,start:end,10])\n",
    "else:\n",
    "    u_test = np.array(ds.u[start:end,lowidx-15])\n",
    "counter = 0                                            # counter to keep track of # of consecutive NaN values\n",
    "nan_list = [0]                                         # empty list to keep track of NaN indices\n",
    "\n",
    "if adcp == 'Shoulder':\n",
    "    limit=1500\n",
    "else:\n",
    "    limit=100\n",
    "\n",
    "for i in range(len(time_test)):                        # loop to count consecutive NaN values\n",
    "    if np.isnan(u_test[i])==True:                      # add to counter if NaN = true\n",
    "        counter += 1\n",
    "        if counter==limit:                               # length of NaN gap\n",
    "            dead = i-limit                               # datetime series hits significant NaN interval\n",
    "            nan_list.append(dead)                      # append end of good data index to nan_list\n",
    "            print('Good data until:',np.max(time_test[dead+1]))\n",
    "        elif counter > limit and i < (len(time_test)-1):   # if NaNs, but before the end of the interval\n",
    "            if np.isnan(u_test[i+1])==False:           # if next value is NOT a NaN\n",
    "                nan_list.append(i+1)                   # append that index to nan_list as start of good data\n",
    "                print('Good data resets at:',time_test[i])\n",
    "    elif np.isnan(u_test[i])==False:                   # reset counter if NaN inconsistent\n",
    "        counter = 0\n",
    "\n",
    "if (len(nan_list)%2) != 0:\n",
    "    nan_list.append(len(time_test))\n",
    "\n",
    "if adcp2 == 'Slope':\n",
    "    u_test = np.array(ds.u[start:end,lowidx:upidx+1])      # total u for interval and depth, to chop\n",
    "    v_test = np.array(ds.v[start:end,lowidx:upidx+1])      # total v for interval and depth, to chop\n",
    "elif adcp2 == 'Axis55':\n",
    "    u_test = np.array(ds.u[0,0,start:end,lowidx:upidx+1])      # total u for interval and depth, to chop\n",
    "    v_test = np.array(ds.v[0,0,start:end,lowidx:upidx+1])      # total v for interval and depth, to chop\n",
    "elif adcp2 == 'Axis75':\n",
    "    u_test = np.array(ds.u[0,0,start:end,lowidx:upidx+1])      # total u for interval and depth, to chop\n",
    "    v_test = np.array(ds.v[0,0,start:end,lowidx:upidx+1])      # total v for interval and depth, to chop\n",
    "else:\n",
    "    u_test = np.array(ds.u[start:end,:])      # total u for interval and depth, to chop\n",
    "    v_test = np.array(ds.v[start:end,:])      # total v for interval and depth, to chop\n",
    "    \n",
    "u_seg = []                                             # empty list for u segment arrays\n",
    "v_seg = []                                             # empty list for v segment arrays\n",
    "t_seg = []                                             # empty list for u segment time intervals\n",
    "\n",
    "for i in range(len(nan_list)):                         # for the number of NaN indices found\n",
    "    if i%2 != 0:                                       # for every second index\n",
    "        u_seg.append(np.array(u_test[nan_list[i-1]:nan_list[i],:]))      # create arrays of good u data\n",
    "        v_seg.append(np.array(v_test[nan_list[i-1]:nan_list[i],:]))      # create arrays of good v data\n",
    "        t_seg.append(np.array(time_test[nan_list[i-1]:nan_list[i]]))     # and their time intervals\n",
    "\n",
    "n_seg = len(u_seg)\n",
    "print('Number of segments:',n_seg)\n",
    "for i in range(n_seg):\n",
    "    print(f'Length of segment {i}:',len(t_seg[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot specified time intervals to check data quality\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(13,5))\n",
    "# for i in range(n_seg):\n",
    "#     im = ax.pcolormesh(t_seg[i], -depth, u_seg[i].T, rasterized=True, cmap='RdBu_r', vmin=-0.15, vmax=0.15, shading='auto')\n",
    "# cbar = fig.colorbar(im, ax=ax, fraction=0.05, pad=0.01, aspect=40, extend='both')\n",
    "# ax.set_xlim(start_date,end_date)\n",
    "# cbar.set_label('Velocity [m/s]')\n",
    "# ax.set_xlabel('Time')\n",
    "# ax.set_ylabel('Depth [m]')\n",
    "# ax.set_title('Velocity data')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust\n",
    "Data rotated based on a visual estimate of along-slope angle, as 30$^{\\circ}$, for Upper Slope. Axis rotation based on lower canyon topography, as %%%. Data are then interpolated to deal with minor instances of NaN values, and filtered using a 40h low-pass Butterworth digital filter to extract the mean currents and residual (tidal) flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# process to rotate, interpolate, and filter raw data (no significant data gaps)\n",
    "\n",
    "uorig,ulp,vorig,vlp = [],[],[],[]   # empty lists for adjusted data\n",
    "\n",
    "for i in range(n_seg):\n",
    "\n",
    "    # rotate Upper Slope data (y is already fairly along-canyon for Axis)\n",
    "    \n",
    "    if adcp == 'Slope' or adcp2 == 'Slope':\n",
    "        theta_along_slope = np.radians(-30)         # rotation angle in radians, -30 degrees to rotate AXES not velocities\n",
    "        u_old = u_seg[i] \n",
    "        v_old = v_seg[i]                                         \n",
    "        u_rot = u_old*np.cos(theta_along_slope) - v_old*np.sin(theta_along_slope)               # u_new = Re(rotated vector)\n",
    "        v_rot = u_old*np.sin(theta_along_slope) + v_old*np.cos(theta_along_slope)               # v_new = Im(rotated vector)     \n",
    "    elif adcp == 'Axis75' or adcp == 'Axis55':\n",
    "        u_rot = u_seg[i]\n",
    "        v_rot = v_seg[i]\n",
    "    elif adcp2 == 'Axis75' or adcp2 == 'Axis55':\n",
    "        u_rot = u_seg[i]\n",
    "        v_rot = v_seg[i]\n",
    "\n",
    "    # filter small NaN instances from data\n",
    "\n",
    "    t = len(t_seg[i])                        # number of time data points after checking for consistent NaN intervals\n",
    "    d = len(depth)                           # number of depth data points after removing unwanted depths\n",
    "\n",
    "    uorig_temp = np.empty([t,d])             # empty array for interpolated & rotated u data\n",
    "    vorig_temp = np.empty([t,d])             # empty array for interpolated & rotated v data\n",
    "\n",
    "    if adcp == 'Shoulder':\n",
    "        limit2 = 1500\n",
    "    else:\n",
    "        limit2 = 100\n",
    "        \n",
    "    for j in range(d):                       # loop to interpolate small gaps at each depth\n",
    "        utemp = pd.Series(u_rot[:,j])\n",
    "        uint = utemp.interpolate(method=\"linear\", limit = limit2, limit_direction='both')\n",
    "        uorig_temp[:,j] = uint               # set interpolated data to original array\n",
    "        vtemp = pd.Series(v_rot[:,j])\n",
    "        vint = vtemp.interpolate(method=\"linear\", limit = limit2, limit_direction='both')\n",
    "        vorig_temp[:,j] = vint               # set interpolated data to original array\n",
    "\n",
    "    # low pass Butterworth filter for 40 hour cut-off to remove 30 hour tides\n",
    "\n",
    "    if adcp == 'Shoulder':\n",
    "        fs1 = 1.667e-2\n",
    "        order = 8\n",
    "        btype1 = 'bandpass'\n",
    "        fc = [1e-4,1e-3]\n",
    "    else:\n",
    "        fs1 = 1.111e-3                            # samples per SECOND for entire time series\n",
    "        order = 8\n",
    "        fc = 6.944e-6\n",
    "        btype1 = 'lowpass'\n",
    "    \n",
    "    sos = sig.butter(N=order, Wn=fc,btype=btype1, fs=fs1, output='sos')   # digital butterworth filter\n",
    "    w, h = sig.sosfreqz(sos)                   # to plot filter response\n",
    "\n",
    "    ulp_temp = np.empty([t,d])               # empty array for low-pass filtered u values\n",
    "    vlp_temp = np.empty([t,d])               # empty array for low-pass filtered v values\n",
    "\n",
    "    for j in range(d):                       # loop for filtered and residual velocities\n",
    "        ufilt = np.copy(uorig_temp[:,j])     # copy unfiltered array\n",
    "        ulp_temp[:,j] = sig.sosfiltfilt(sos, ufilt)  # apply low pass filter\n",
    "\n",
    "        vfilt = np.copy(vorig_temp[:,j])             # repeat for v\n",
    "        vlp_temp[:,j] = sig.sosfiltfilt(sos, vfilt)\n",
    "        \n",
    "    uorig.append(uorig_temp)                  # append cleaned data to lists\n",
    "    vorig.append(vorig_temp)\n",
    "    ulp.append(ulp_temp)\n",
    "    vlp.append(vlp_temp)\n",
    "    \n",
    "if adcp == 'Shoulder' and adcp2 == 'Axis75' and year == 2013:\n",
    "    uorig[0][-5000:,:] = np.nan\n",
    "    vorig[0][-5000:,:] = np.nan\n",
    "    uorig[1][165000:171000,:] = np.nan\n",
    "    vorig[1][165000:171000,:] = np.nan\n",
    "if adcp == 'Shoulder' and adcp2 == 'Axis55' and year == 2017:\n",
    "    uorig[0][165000:171000,:] = np.nan\n",
    "    vorig[0][165000:171000,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# plot filter response\n",
    "\n",
    "#plt.semilogx(w*fs/(2*np.pi),abs(h))\n",
    "#plt.semilogx(w/np.pi,abs(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot specified time intervals to check data quality\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(13,5))\n",
    "# for i in range(n_seg):\n",
    "#     im = ax.pcolormesh(t_seg[i], -depth, uorig[i].T, rasterized=True, cmap='RdBu_r', vmin=-0.15, vmax=0.15, shading='auto')\n",
    "# cbar = fig.colorbar(im, ax=ax, fraction=0.05, pad=0.01, aspect=40, extend='both')\n",
    "# ax.set_xlim(start_date,end_date)\n",
    "# cbar.set_label('Velocity [m/s]')\n",
    "# ax.set_xlabel('Time')\n",
    "# ax.set_ylabel('Depth [m]')\n",
    "# ax.set_title('Velocity data')\n",
    "# date_form = pldates.DateFormatter(\"%m/%d\")\n",
    "# ax.xaxis.set_major_formatter(date_form)\n",
    "# if adcp2 == 'Axis75' and t_stamp == '2013':\n",
    "#     ax.set_xlim(np.datetime64(f'{t_stamp}-05-01'),np.datetime64(f'{t_stamp}-05-30'))\n",
    "#     #ax.set_xlim(None,None)\n",
    "# else:\n",
    "#     ax.set_xlim(None,None)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "Save key values and arrays to NetCDF format using xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save to .nc files\n",
    "\n",
    "for i in range(n_seg):\n",
    "    ds_out = xr.Dataset( \n",
    "        data_vars=dict(\n",
    "            uorig=(['t_seg','depth'], uorig[i]),    # adjusted data\n",
    "            vorig=(['t_seg','depth'], vorig[i]),\n",
    "            ulp=(['t_seg','depth'], ulp[i]),        # low-pass data\n",
    "            vlp=(['t_seg','depth'], vlp[i]),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            depth=depth,                 # depth values\n",
    "            t_seg=t_seg[i],              # datetime values in segments\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=f'Adjusted data for {adcp} {t_stamp} segment {i} (from 0 to {n_seg - 1}).',\n",
    "            adcp=adcp,                   # adcp\n",
    "            upidx=upidx,                 # upper depth index\n",
    "            lowidx=lowidx,               # lower depth index\n",
    "            dup_stamp=dup_stamp,         # upper depth stamp\n",
    "            dlow_stamp=dlow_stamp,       # lower depth stamp\n",
    "            t_stamp=t_stamp,             # year stamp\n",
    "            start_date=f'{start_date}',  # start date\n",
    "            end_date=f'{end_date}',      # end date\n",
    "            t=t,                         # length of time series\n",
    "            d=d,                         # length of depth series\n",
    "            n_seg=n_seg,                 # total number of segments\n",
    "        ),\n",
    "    ) \n",
    "    if adcp == 'Shoulder':\n",
    "        ds_out.to_netcdf(f'../Data/data/adj/adj_{adcp2}_1min_{t_stamp}_{i}.nc')\n",
    "    else:\n",
    "        ds_out.to_netcdf(f'../Data/data/adj/adj_{adcp}_{t_stamp}_{i}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "16",
    "lenType": "16",
    "lenVar": "200"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "634px",
    "left": "513px",
    "right": "20px",
    "top": "97px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
